{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning in Medicine\n",
    "## BMSC-GA 4493, BMIN-GA 3007\n",
    "## Homework 3: Sequence Classification\n",
    "\n",
    "Note 1: If you don't know how to run jupyter on the Prince cluster, here is another step-by-step guide here: \n",
    "<a href='https://docs.google.com/document/d/1HIdtzqJ6-RpsV0z2Gf5iXphNBTRca1kHZPlyqFxKpWs/edit?usp=sharing'> **Running Jupyter on the Cluster **</a>\n",
    "\n",
    "Note 2: If you need to write mathematical terms, you can type your answeres in a Markdown Cell via LaTex\n",
    "See: <a href=\"https://stackoverflow.com/questions/13208286/how-to-write-latex-in-ipython-notebook\">here</a> if you have issues. To see basic LaTex notation see: <a href=\"https://en.wikibooks.org/wiki/LaTeX/Mathematics\">here</a>.\n",
    "\n",
    "\n",
    "Submission instruction: Upload and Submit your final jupyter notebook file in newclasses.nyu.edu\n",
    "\n",
    "Submission deadline: Tuesday April 10th 2018 (3pm, before class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Literature Review for Sequence Classification: DeepBind (Total points 20 + 20 Bonus points)\n",
    "\n",
    "Read this paper:\n",
    "\n",
    "#### Babak Alipanahi1, Andrew Delong, Matthew T Weirauch & Brendan J Frey, *\"Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning\"* Nature Biotechnology, 2015  https://www.nature.com/articles/nbt.3300.pdf\n",
    "\n",
    "We are interested in understanding the task, the methods that is proposed in this publication, technical aspects of the implementation, and possible future work.\n",
    "\n",
    "**1.1) (5 points)** After you read the full article, go back to section **Training DeepBind and scoring sequences**. How do the authors define the **binding score, f(s)** on a given sequence? Write the formula here, and describe in your own words, in a few sentences, how the score is computed for each sequence (s)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$f(s)=net_w(pool(rect_b(conv_m(s))))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S is the sequence under consideration. Convolution operation is performed on S using filter ( MOtif detector with parameter M across the sequence). These filters are of size 4*M.\n",
    "\n",
    "Later, on the covolution result, rectification operation is performed. Where the positions are isolated with good patern match by shifting response of motif detector $M_k$ by $b_k$. Also it clamps all the negetive values to zero.\n",
    "\n",
    "On the output of rectification output, pooling is performed. Both the average and max pooling are peeformed ad max pooling is helpful in detecting longer motifs and average pooling helps in cummulative effect of smalled motifs.\n",
    "\n",
    "These values are later fed into a non linear neural network with weights - W. This would produce the score $f(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2) (5 points)** What is the loss function that they are optimizing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3) (5 points)** What is the evaluation criteria based on which the authors do their cross validation? (Hint: Check Figure 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4) (5 points)** Are there some data augmentation/regularization that authors have used? What are some techniques that could have been used and wasn't? (Go back to Lecture 7 or Chapter 7 of your book for more info.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5) (Bonus maximum 20 points)**. What other architectures would you try? For each family of models, please do a literature search and see if a paper on that architecture for the task of DNA binding detection has been "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - Literature Review for Sequence Labeling (20 points)\n",
    "\n",
    "Read this paper: \n",
    "\n",
    "\n",
    "#### Mohammed AlQuraishi, \"End-to-end differentiable learning of protein structure\", 2018 bioRxiv 265231; doi: https://doi.org/10.1101/265231\n",
    "\n",
    "\n",
    "We are interested in the task, the methods proposed in the paper, technical aspect of the implementation, and possible future work. \n",
    "\n",
    "After you read the article, go back to Figure 2. \n",
    "\n",
    "**2.1)( 5 points)** What is the architecture used in this task, to predict from the amino acid letter, to the three torsion angles? Describe the family of the architecture and few words on how the input sequence is converted to output in that architecture?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2)(5 points)** Once the structure is predicted, what is the Loss function that is being optimized between the predicted structure and the ground truth structure? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3)(10 points) ** What are some alternative architectures that you would recommend as followup work? Name 2 potential architectures, and in a few sentences explain why the proposed model might work better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4) - no points just for your reference : The dataset for this paper is publicly available. Any new architecture that improves the prediction model will definitely be publication worthy and extremely valuable! Make a note of that and if you work on this topic, check the dataset out: https://github.com/aqlaboratory/proteinnet **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 - Programming: Build Sequence Classifiers - Convolutional and Recurrent (60 points + 5 bonus points)\n",
    "\n",
    "Let's build some models now, to try to classify each <a href=\"\">protein</a> (represented as <a href=\"https://en.wikipedia.org/wiki/Protein_primary_structure\">a sequence of amino acids</a>), into protein families.  \n",
    "\n",
    "Why this is an important task? Briefly, our DNAs encode the code for proteins, which are molecular machines that make the cells work. \n",
    "\n",
    "![Our DNAs encode the code for proteins, which are molecular machines that make the cells work](https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/Genetic_code.svg/580px-Genetic_code.svg.png) | ![Sequence to Structure](http://www.robotics.tu-berlin.de/fileadmin/_processed_/1/1f/csm_compbio_seq2struct_1614a2532b.jpg)\n",
    "\n",
    "Given the sequence of the amino acids, there is great scientific value in being able to predict its 3D structure, and predict whether the protein will or will not bind to other chemical molecules such as drugs or other proteins. \n",
    "The applications are numerous in disease understanding and treatment (i.e. <a href=\"https://en.wikipedia.org/wiki/Amyloid_beta\">Alzheimer's disease is related to *beta-amyloid* proteins in our brain not folding correctly and creating plaques</a>).\n",
    "\n",
    "In this homework, we will focus on a dataset which has more than 400,000 protein sequences and their classes. The data and related pre-processing scriptes are is available <a href=\"https://www.kaggle.com/abharg16/predicting-protein-classification/data\">here</a> and <a href=\"https://www.kaggle.com/abharg16/predicting-protein-classification/notebook\">here</a>, which are super awesome.\n",
    "\n",
    "\n",
    "Here, we will focus on predicting top 3 classes of proteins, from the sequence of the amino acids of that protein.\n",
    "The data is available in the cluster in /scratch/nsr3/protein/rcsb/, although you're also welcome to have your own local copy of the data and work with that. We need two files: pdb_data_seq.csv and pdb_data_no_dups.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.1) (5 points)** Preprocessing. Most of the preprocessing is available in the kernel that came with the data. In paricular you can use the following to pre-process your data. Pre-process your data and tell us, \n",
    "how many data samples are available after the pre-processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import Datasets\n",
    "df_seq = pd.read_csv('/scratch/nsr3/protein/rcsb/pdb_data_seq.csv')\n",
    "df_char = pd.read_csv('/scratch/nsr3/protein/rcsb/pdb_data_no_dups.csv')\n",
    "\n",
    "# Filter for only proteins\n",
    "protein_char = df_char[df_char.macromoleculeType == 'Protein']\n",
    "protein_seq = df_seq[df_seq.macromoleculeType == 'Protein']\n",
    "\n",
    "# Select only necessary variables to join\n",
    "protein_char = protein_char[['structureId','classification']]\n",
    "protein_seq = protein_seq[['structureId','sequence']]\n",
    "\n",
    "model_f = protein_char.set_index('structureId').join(protein_seq.set_index('structureId'))\n",
    "model_f = model_f.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.2) (5 points) ** Select only the classes that have *more than 30,000 samples*. Only keep the rows that belong to one of these three classes in your data. Which classes are there, and how many rows do you have after this filteration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.3) (5 points) ** Write a function, that takes a protein sequence *S* in, and converts it into a numpy array of size *25 x Len(S)*, which has the *one-hot encoding of the sequence*. \n",
    "\n",
    "I.e. For each amino acid s_i in the sequence, we put a 1.0 in the row corresponding to the index of that amino acid in the alphabet (see an alphabet example below), and we put 0.0 in every other row letter position. \n",
    "\n",
    "You can use this list as all possible Amino Acid letters: **['H','V','G','A','P','C','D','I','R','E','K','L','W','T','Y','S','Q','F','N','M','U','X','Z','B','O']**\n",
    "\n",
    "As another example, if S_0 is an 'H', the first column of our returned results has a 1.0 in row number 0 and, 0.0 in every other row. If it is a S_1 is a 'G', we put a 1.0 in row number 2 of that column, and a 0.0 in every other row in that column. We continue for all letterse in our sequence. \n",
    "\n",
    "You can use the following function as a utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_alphabet_index():\n",
    "    alphabet_map = {}\n",
    "    for ix,letter in enumerate(['H','V','G','A','P','C','D','I','R','E','K','L','W','T','Y','S','Q','F','N','M','U','X','Z','B','O']):\n",
    "        alphabet_map[letter] = ix\n",
    "    return alphabet_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fill-in the below function to on-hot-encode your proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seqstring_to_seqbinary(seqLetters):\n",
    "    #\n",
    "    # FILL IN YOUR FUNCTION HERE\n",
    "    #\n",
    "    return seq_one_hot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4) (5 points)** Now convert your data into train, test and validation set. Shuffle the rows, and split them with ratios of (train:60%, valid:20%, test:20%).\n",
    "\n",
    "(Hint: it's useful to set the random number seed before shuffling, so you get the same results over multiple runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5) (5 points)** Now, convert your training, validation and test sequences to one-hot numpy arrays. \n",
    "Doing so in advance will save you computation time later. Also since we will be training a classifier, convert your one-hot label variables into the index. i.e. if your label is [0, 1, 0] convert it into [1]. If it is [0, 0, 1], convert it into [2]. (Hint: Use *numpy argmax* method if needed for fast implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.6) (10 points + 5 bonus points) Now you are ready to build your sequence classification model! **\n",
    "\n",
    "\n",
    "First, build a Convolutional sequence classification model similar to the architecture in question 1, (deepbind paper). \n",
    "\n",
    "Use Convolution, negative log likelihood (NLL) loss, and (optional: any additions to your architecture!), to go from the one-hot sequence of size *25 x len(S)* to 3-class classifier. \n",
    "\n",
    "At each epoch, compute **Average NLL loss** and **one AUC score per class i.e. 3 AUC scores** on both **train and validation set** (hint: look at solutions to <a href=\"https://github.com/nyumc-dl/BMSC-GA-4493-Spring2018/blob/master/hw1/hw1-solutions.ipynb\">HW1</a> Q.4.2 for AUC and model details. Pytorch is very sensative to TensorTypes so you need to make sure you give the right data type and data shape, to Loss function and your model)\n",
    "\n",
    "Plot your validation and train loss over different epochs, and also print the AUCs on train and validation sets.\n",
    "\n",
    "\n",
    "**(Bonus 5 points) Switch everything to Cuda.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.7) (5 points) ** The benefit of convolutional sequence model is that they are easier to interpret later. \n",
    "Use matplotlib and plt.imshow(), to visualize the filters of the first layer convolution that you have: \n",
    "\n",
    "(hint: an example, if the model is named model and the first layer of convolution is accessible via model.convnet1, the following code can give you those filters:\n",
    "kernels = [k[0].data.numpy() for k in model.convnet1.weight])\n",
    "\n",
    "**Note: It's ok if your model didn't converge at all. Just show the visualizations!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.8)(10 points)** Now, provide a second sequence classification model based on LSTMs. Build a simple LSTM model that takes as input the (25 x Len(s)) array, and ends with a softmax over 3 classees. (Hint - you probably need to *transpose* your data to a shape Len(s) x 25, to give it to LSTM. Up to you whether you will do that in advance or during the processing of the LSTM).\n",
    "\n",
    "The rest of your experimental setting should be the same as section 3.6. \n",
    "\n",
    "At each epoch, compute Average NLL loss and one AUC score per class i.e. 3 AUC scores on both train and validation set.\n",
    "\n",
    "Plot your validation and train loss over different epochs, and also print the AUCs on train and validation sets.\n",
    "\n",
    "NOTE: At some point in your back-propagation, you may get an error which needs you to set a parameter retain_graph to be true, to solve. Advance warnings if you encounter that, and your solution will be: loss.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**3.9) (5 points)** What are some other architectures that you could be using in future work? List a few and in a few sentences discuss why they might be a good fit for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.10)(5 points)** What are some other fine-tunning/regularizations/etc. that you could do in the future work, to improve the scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
